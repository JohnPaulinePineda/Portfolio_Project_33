---
title: 'R : Exploring and Visualizing Extracted Dimensions from Principal Component Algorithms'
author: "John Pauline Pineda"
date: "June 02, 2023"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```
# **1. Table of Contents**
|
| This document explores methods in visualizing extracted dimensions from principal component algorithms using various helpful packages in <mark style="background-color: #CCECFF">**R**</mark>.    
|
| Dimensionality reduction is a form of unsupervised learning method aimed at reducing the number of features in a data set while retaining as much information as possible for purposes of lessening the complexity of the model, improving the performance of the learning algorithm or for formulating a more intuitive visualization of the data. The algorithms applied in this study (mostly contained in the <mark style="background-color: #CCECFF">**factoextra**</mark>, <mark style="background-color: #CCECFF">**stats**</mark> and <mark style="background-color: #CCECFF">**FactoMineR**</mark> packages) attempt to transform and project the data onto a lower-dimensional space while exploring visualization alternatives for the extracted dimensions.
|
##  1.1 Sample Data
|
| The [<mark style="background-color: #EEEEEE;color: #FF0000">**Oscars**</mark>](https://www.kaggle.com/datasets/unanimad/the-oscar-award), [<mark style="background-color: #EEEEEE;color: #FF0000">**Tomatometer**</mark>](https://www.kaggle.com/datasets/andrezaza/clapper-massive-rotten-tomatoes-movies-and-reviews) and [<mark style="background-color: #EEEEEE;color: #FF0000">**IMDB**</mark>](https://www.kaggle.com/datasets/sankha1998/tmdb-top-10000-popular-movies-dataset) datasets obtained from the  <mark style="background-color: #CCECFF">**Kaggle**</mark> website were used for this illustrated example.   
|
| Preliminary dataset assessment:
|
| **[A]** 126 rows (observations)
| 
| **[B]** 19 columns (variables)
|      **[B.1]** 1/19 instance = <span style="color: #FF0000">Film</span> variable (character)
|      **[B.2]** 1/19 labels = <span style="color: #FF0000">Year</span> variable (factor)
|      **[B.3]** 1/19 labels = <span style="color: #FF0000">Picture</span> variable (factor)
|             **[B.3.1]** Category 1 = <span style="color: #FF0000">Picture=WON</span> 
|             **[B.3.2]** Category 2 = <span style="color: #FF0000">Picture=NOM</span> 
|      **[B.4]** 16/19 descriptors = 8/16 numeric + 8/16 factor 
|             **[B.4.1]** <span style="color: #FF0000">Tomatometer_Critic</span> (numeric)
|             **[B.4.2]** <span style="color: #FF0000">Tomatometer_Audience</span> (numeric)
|             **[B.4.3]** <span style="color: #FF0000">Tomatometer_Critic_Audience_Gap</span> (numeric)
|             **[B.4.4]** <span style="color: #FF0000">IMDB_Critic</span> (numeric)
|             **[B.4.5]** <span style="color: #FF0000">IMDB_Audience</span> (numeric)
|             **[B.4.6]** <span style="color: #FF0000">IMDB_Critic_Audience_Gap</span> (numeric)
|             **[B.4.7]** <span style="color: #FF0000">Nominations_Total</span> (numeric)
|             **[B.4.8]** <span style="color: #FF0000">Nomination_SuccessRate</span> (numeric)
|             **[B.4.9]** <span style="color: #FF0000">Genre</span> (factor)
|             **[B.4.10]** <span style="color: #FF0000">Cinematography</span> (factor)
|             **[B.4.11]** <span style="color: #FF0000">Directing</span> (factor)
|             **[B.4.12]** <span style="color: #FF0000">Editing</span> (factor)
|             **[B.4.13]** <span style="color: #FF0000">Screenplay</span> (factor)
|             **[B.4.14]** <span style="color: #FF0000">Acting</span> (factor)
|             **[B.4.15]** <span style="color: #FF0000">Design</span> (factor)
|             **[B.4.16]** <span style="color: #FF0000">Sound</span> (factor)
|     
| 
```{r section_1.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(AppliedPredictiveModeling)
library(caret)
library(rpart)
library(lattice)
library(dplyr)
library(tidyr)
library(moments)
library(skimr)
library(RANN)
library(pls)
library(corrplot)
library(tidyverse)
library(lares)
library(DMwR)
library(gridExtra)
library(rattle)
library(RColorBrewer)
library(stats)
library(factoextra)
library(FactoMineR)

##################################
# Loading source and
# formulating the analysis set
##################################
Oscars <- read.csv("Oscars.csv",
                   na.strings=c("NA","NaN"," ",""),
                   stringsAsFactors = FALSE)
Oscars <- as.data.frame(Oscars)

##################################
# Performing a general exploration of the data set
##################################
dim(Oscars)
str(Oscars)
summary(Oscars)

##################################
# Transforming to appropriate data types
##################################
Oscars$Year <- factor(Oscars$Year,
                      levels = c("2010",
                                 "2011",
                                 "2012",
                                 "2013",
                                 "2014",
                                 "2015",
                                 "2016",
                                 "2017",
                                 "2018",
                                 "2019",
                                 "2020",
                                 "2021",
                                 "2022",
                                 "2023"))

Oscars$Genre          <- as.factor(Oscars$Genre)
Oscars$Picture        <- as.factor(Oscars$Picture)
Oscars$Cinematography <- as.factor(Oscars$Cinematography)
Oscars$Directing      <- as.factor(Oscars$Directing)
Oscars$Editing        <- as.factor(Oscars$Editing)
Oscars$Screenplay     <- as.factor(Oscars$Screenplay)
Oscars$Acting         <- as.factor(Oscars$Acting)
Oscars$Design         <- as.factor(Oscars$Design)
Oscars$Sound          <- as.factor(Oscars$Sound)

##################################
# Formulating a data type assessment summary
##################################
PDA <- Oscars
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)

```

##  1.2 Data Quality Assessment
|
| Data quality assessment:
|
| **[A]** No missing observations noted for any variable.
|
| **[B]** No low variance observed for any variable with First.Second.Mode.Ratio>5.
|
| **[C]** No low variance observed for any variable with Unique.Count.Ratio<0.01.
|
| **[D]** High skewness observed for 14 variables with Skewness>3 or Skewness<(-3).
|
```{r section_1.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- Oscars[,c(3:11,13:19)]

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Index=c(1:length(names(DQA))),
  Column.Name= names(DQA),
  Column.Type=sapply(DQA, function(x) class(x)),
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all descriptors
##################################
DQA.Descriptors <- DQA

##################################
# Listing all numeric Descriptors
##################################
DQA.Descriptors.Numeric <- DQA.Descriptors[,sapply(DQA.Descriptors, is.numeric)]

if (length(names(DQA.Descriptors.Numeric))>0) {
    print(paste0("There are ",
               (length(names(DQA.Descriptors.Numeric))),
               " numeric descriptor variable(s)."))
} else {
  print("There are no numeric descriptor variables.")
}

##################################
# Listing all factor Descriptors
##################################
DQA.Descriptors.Factor <- DQA.Descriptors[,sapply(DQA.Descriptors, is.factor)]

if (length(names(DQA.Descriptors.Factor))>0) {
    print(paste0("There are ",
               (length(names(DQA.Descriptors.Factor))),
               " factor descriptor variable(s)."))
} else {
  print("There are no factor descriptor variables.")
}

##################################
# Formulating a data quality assessment summary for factor Descriptors
##################################
if (length(names(DQA.Descriptors.Factor))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Descriptors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Descriptors.Factor),
  Column.Type=sapply(DQA.Descriptors.Factor, function(x) class(x)),
  Unique.Count=sapply(DQA.Descriptors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Descriptors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Descriptors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Descriptors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Descriptors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Descriptors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Descriptors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Descriptors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Formulating a data quality assessment summary for numeric Descriptors
##################################
if (length(names(DQA.Descriptors.Numeric))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Descriptors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Descriptors.Numeric),
  Column.Type=sapply(DQA.Descriptors.Numeric, function(x) class(x)),
  Unique.Count=sapply(DQA.Descriptors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Descriptors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Descriptors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Descriptors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Descriptors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Descriptors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Descriptors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Descriptors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Descriptors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Descriptors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Descriptors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Descriptors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Descriptors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Descriptors.Numeric, function(x) format(round(kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Descriptors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Descriptors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance Descriptors
##################################
if (length(names(DQA.Descriptors.Factor))==0) {
  print("No factor descriptors noted.")
} else if (nrow(DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor descriptors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric descriptors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric descriptors due to low unique count ratio noted.")
}

##################################
# Checking for skewed Descriptors
##################################
if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric descriptors noted.")
}
```

##  1.3 Data Preprocessing

###  1.3.1 Dataset Formulation
|
```{r section_1.3.1, warning=FALSE, message=FALSE}
##################################
# Formulating dataset versions
# for the different principal component algorithms
##################################

##################################
# Formulating dataset for
# Principal Component Analysis
##################################

Oscars.PCA <- Oscars[,c("Tomatometer_Critic",
                        "Tomatometer_Audience",
                        "IMDB_Critic",
                        "IMDB_Audience",
                        "Nominations_Total",
                        "Nomination_SuccessRate",
                        "Year",
                        "Picture")]
row.names(Oscars.PCA) <- Oscars$Film
dim(Oscars.PCA)
str(Oscars.PCA)
summary(Oscars.PCA)


```

## 1.4 Data Exploration
|
```{r section_1.4, warning=FALSE, message=FALSE}

```

## 1.5 Dimensionality Reduction

###  1.5.1 Principal Component Analysis (PCA)
|
| [Principal Component Analysis](https://psycnet.apa.org/doiLanding?doi=10.1037%2Fh0071325) employs a linear transformation that is based on preserving the most variance in the data using the least number of dimensions. The original data set in a higher dimensional space is mapped to a lower dimension space with maximum variance. The process involves the construction of the covariance matrix of the original data set. The eigenvectors of the covariance matrix of the data are referred to as principal axes, and the projection of the data instances on to these principal axes are called the principal components. Dimensionality reduction is obtained by only retaining the axes (dimensions) that account for most of the variance, and discarding all others.
|
```{r section_1.5.1, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
SD_PCA <- Oscars.PCA

SD_PCA.Numeric <- Oscars.PCA[,sapply(Oscars.PCA, is.numeric)]
colnames(SD_PCA.Numeric)
  
##################################
# Performing PCA
##################################
DR_PCA <- PCA(SD_PCA[,c(1:6)],
              scale.unit = TRUE,
              graph = FALSE)

##################################
# Obtaining the PCA Eigenvalues
##################################
(DR_PCA_EV <- get_eigenvalue(DR_PCA))

##################################
# Formulating the Scree Plot
##################################
(DR_PCA_VariableScreePlot <- fviz_eig(DR_PCA, addlabels = TRUE, ylim = c(0, 100)) +
  labs(title = "PCA : Scree Plot",
       subtitle = "Top 2 Principal Components",
       y = "Percentage of Explained Variances",
       x = "Principal Components") +
  theme_classic())

##################################
# Extracting the PCA active variables
##################################
DR_PCA_VAR <- get_pca_var(DR_PCA)

##################################
# Extracting the coordinates
# for the descriptor variables
##################################
DR_PCA_VAR$coord

##################################
# Extracting the correlations 
# between the descriptor variables
# and principal component dimensions
##################################
DR_PCA_VAR$cor

##################################
# Extracting the quality of representation
# for the descriptor variables
# on the factor map 
##################################
DR_PCA_VAR$cos2

##################################
# Extracting the contributions 
# (in percentage) of the descriptor variables
# to the principal components
##################################
DR_PCA_VAR$contrib

##################################
# Formulating clusters of the
# descriptor variables
##################################
set.seed(123)
DR_PCA_KMEANS <- kmeans(DR_PCA_VAR$coord, centers=3, nstart=25)
DR_PCA_KMEANS_CLUSTER <- as.factor(DR_PCA_KMEANS$cluster) 

##################################
# Extracting the correlation 
# between the descriptor variables
# and top principal components
##################################
(DR_PCA_VariableCorrelationCircle <- fviz_pca_var(DR_PCA, 
                                         col.var = DR_PCA_KMEANS_CLUSTER, 
                                         palette = c("#0073C2FF", "#EFC000FF", "#868686FF"),
                                         legend.title = "Cluster") +
  labs(title = "PCA : Correlation Circle",
       subtitle = "Principal Components Versus Descriptor Variable Clusters",
       y = "Principal Component 2",
       x = "Principal Component 1") +
  theme_classic() +
  theme(legend.position="top"))

##################################
# Extracting the quality of representation 
# for the descriptor variables
# on the factor map 
##################################
(DR_PCA_VariableSquaredCorrelationCircle <- fviz_pca_var(DR_PCA, 
                                         col.var = "cos2", 
                                         gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                                         legend.title = "Squared Coordinates") +
  labs(title = "PCA : Squared Correlation Circle",
       subtitle = "Descriptor Variabble Representation Quality",
       y = "Principal Component 2",
       x = "Principal Component 1") +
  theme_classic() +
  theme(legend.position="top"))

##################################
# Extracting the contribution 
# of the descriptor variables
# for the the Top 1 and 2 principal components
##################################
(DR_PCA_PCVariableContributors <- fviz_contrib(DR_PCA, 
                                 choice = "var", 
                                 axes = 1:2) +
  labs(title = "PCA : Descriptor Variable Contribution",
       subtitle = "Principal Components 1 and 2 Contributors",
       x = "Descriptors") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90,hjust = 1)))

##################################
# Testing the statistical significance
# of the contribution 
# of the descriptor variables
# for the the Top 1 and 2 principal components
##################################
DR_PCA_VariableContributionTest <- dimdesc(DR_PCA, 
                                           axes = c(1,2), 
                                           proba = 0.05)

##################################
# Extracting the contribution 
# of the descriptor variables
# for the Top 1 principal component
##################################
(DR_PCA_PC1VariableContributors <- fviz_contrib(DR_PCA, 
                                 choice = "var", 
                                 axes = 1) +
  labs(title = "PCA : Descriptor Variable Contribution",
       subtitle = "Principal Component 1 Contributors",
       x = "Descriptors") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90,hjust = 1)))

##################################
# Testing the statistical significance
# of the contribution 
# of the descriptor variables
# for the Top 1 principal component
##################################
DR_PCA_VariableContributionTest$Dim.1

##################################
# Extracting the contribution 
# of the descriptor variables
# for the Top 2 principal component
##################################
(DR_PCA_PC2VariableContributors <- fviz_contrib(DR_PCA, 
                                 choice = "var", 
                                 axes = 2) +
  labs(title = "PCA : Descriptor Variable Contribution",
       subtitle = "Principal Component 2 Contributors",
       x = "Descriptors") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90,hjust = 1)))

##################################
# Testing the statistical significance
# of the contribution 
# of the descriptor variables
# for the Top 2 principal component
##################################
DR_PCA_VariableContributionTest$Dim.2

##################################
# Extracting the PCA active individuals
##################################
DR_PCA_IND <- get_pca_ind(DR_PCA)

##################################
# Extracting the coordinates
# for the individuals
##################################
DR_PCA_IND$coord

##################################
# Extracting the quality of representation
# for the individuals
# on the factor map 
##################################
DR_PCA_IND$cos2

##################################
# Extracting the contributions 
# (in percentage) of the individuals
# to the principal components
##################################
DR_PCA_IND$contrib

##################################
# Extracting the correlation 
# between the individual instances
# grouped by Picture categories
# and top principal components
##################################
(DR_PCA_IndividualCorrelationCircleByPicture <- fviz_pca_ind(DR_PCA,
             geom.ind = "text",
             col.ind = SD_PCA$Picture,
             repel = TRUE, 
             legend.title = "Picture") +
  labs(title = "PCA : Correlation Circle",
       subtitle = "Principal Components Versus Individuals Grouped by Picture Categories",
       y = "Principal Component 2",
       x = "Principal Component 1") +
  theme_classic() +
  theme(legend.position="top"))

##################################
# Extracting the correlation 
# between the individual instances
# grouped by Year categories
# and top principal components
##################################
(DR_PCA_IndividualCorrelationCircleByYear <- fviz_pca_ind(DR_PCA,
             geom.ind = "text",
             col.ind = SD_PCA$Year,
             repel = TRUE, 
             legend.title = "Year") +
  labs(title = "PCA : Correlation Circle",
       subtitle = "Principal Components Versus Individuals Grouped by Picture Categories",
       y = "Principal Component 2",
       x = "Principal Component 1") +
  theme_classic() +
  theme(legend.position="top"))

##################################
# Extracting the contribution 
# of the descriptor variables
# for the Top 1 and Top 2 principal components
##################################
(DR_PCA_PCIndividualContributors <- fviz_contrib(DR_PCA, 
              choice = "ind", 
              axes = 1:2, 
              top = 10) +
  labs(title = "PCA : Individual Contribution",
       subtitle = "Principal Components 1 ad 2 Contributors",
       x = "Descriptors") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90,hjust = 1)))

##################################
# Extracting the contribution 
# of the descriptor variables
# for the Top 1 principal component
##################################
(DR_PCA_PC1IndividualContributors <- fviz_contrib(DR_PCA, 
              choice = "ind", 
              axes = 1, 
              top = 10) +
  labs(title = "PCA : Individual Contribution",
       subtitle = "Principal Component 1 Contributors",
       x = "Descriptors") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90,hjust = 1)))

##################################
# Extracting the contribution 
# of the descriptor variables
# for the Top 2 principal component
##################################
(DR_PCA_PC2IndividualContributors <- fviz_contrib(DR_PCA, 
              choice = "ind", 
              axes = 2, 
              top = 10) +
  labs(title = "PCA : Individual Contribution",
       subtitle = "Principal Component 2 Contributors",
       x = "Descriptors") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90,hjust = 1)))

```

###  1.5.2 Correspondence Analysis (CA)
|
```{r section_1.5.2, warning=FALSE, message=FALSE}

```

###  1.5.3 Multiple Correspondence Analysis (MCA)
|
```{r section_1.5.3, warning=FALSE, message=FALSE}

```

###  1.5.4 Factor Analysis of Mixed Data (FAMD)
|
```{r section_1.5.4, warning=FALSE, message=FALSE}

```

###  1.5.5 Multiple Factor Analysis (MFA)
|
```{r section_1.5.5, warning=FALSE, message=FALSE}

```

##  1.6 Algorithm Comparison Summary
|
```{r section_1.6, warning=FALSE, message=FALSE}

```

# **2. References**
|
| **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson
| **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani
| **[Book]** [Multivariate Data Visualization with R](http://lmdvr.r-forge.r-project.org/figures/figures.html) by Deepayan Sarkar
| **[Book]** [Machine Learning](https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/) by Samuel Jackson
| **[Book]** [Data Modeling Methods](https://bookdown.org/larget_jacob/data-modeling-methods/) by Jacob Larget
| **[Book]** [Introduction to R and Statistics](https://saestatsteaching.tech/) by University of Western Australia
| **[Book]** [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by Max Kuhn and Kjell Johnson
| **[Book]** [Introduction to Research Methods](https://bookdown.org/ejvanholm/Textbook/) by Eric van Holm
| **[R Package]** [AppliedPredictiveModeling](https://cran.r-project.org/web//packages/AppliedPredictiveModeling/AppliedPredictiveModeling.pdf) by Max Kuhn
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [rpart](https://mran.microsoft.com/web/packages/rpart/rpart.pdf) by Terry Therneau and Beth Atkinson
| **[R Package]** [lattice](https://cran.r-project.org/web/packages/lattice/lattice.pdf) by  Deepayan Sarkar
| **[R Package]** [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html/) by Hadley Wickham
| **[R Package]** [moments](https://cran.r-project.org/web/packages/moments/index.html) by Lukasz Komsta and Frederick
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [RANN](https://cran.r-project.org/web/packages/RANN/RANN.pdf) by  Sunil Arya, David Mount, Samuel Kemp and Gregory Jefferis
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [tidyverse](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf) by Hadley Wickham
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [DMwR](https://mran.microsoft.com/snapshot/2016-05-02/web/packages/DMwR/DMwR.pdf) by Luis Torgo
| **[R Package]** [gridExtra](https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf) by Baptiste Auguie and Anton Antonov
| **[R Package]** [rattle](https://cran.r-project.org/web/packages/rattle/rattle.pdf) by Graham Williams
| **[R Package]** [RColorBrewer](https://cran.r-project.org/web//packages/RColorBrewer/RColorBrewer.pdf) by Erich Neuwirth
| **[R Package]** [stats](https://search.r-project.org/R/refmans/stats/html/00Index.html) by R Core Team
| **[R Package]** [factoextra](https://cran.r-project.org/web/packages/factoextra/factoextra.pdf) by Alboukadel Kassambara and Fabian Mundt
| **[R Package]** [FactoMineR](https://search.r-project.org/R/refmans/stats/html/00Index.html) by Francois Husson, Julie Josse, Sebastien Le and Jeremy Mazet
| **[Article]** [6 Dimensionality Reduction Techniques in R (with Examples)](https://cmdlinetips.com/2022/07/dimensionality-reduction-techniques-in-r/) by CMDLineTips Team
| **[Article]** [6 Dimensionality Reduction Algorithms With Python](https://machinelearningmastery.com/dimensionality-reduction-algorithms-with-python/) by Jason Brownlee
| **[Article]** [Introduction to Dimensionality Reduction for Machine Learning](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/) by Jason Brownlee
| **[Article]** [Introduction to Dimensionality Reduction](https://www.geeksforgeeks.org/dimensionality-reduction/) by Geeks For Geeks
| **[Article]** [Principal Component Analysis for Dimensionality Reduction in Python](https://machinelearningmastery.com/principal-components-analysis-for-dimensionality-reduction-in-python/) by Jason Brownlee
| **[Article]** [Principal Component Analysis Explained Simply](https://blog.bioturing.com/2018/06/14/principal-component-analysis-explained-simply/) by Linh Ngo
| **[Article]** [A Step-by-Step Explanation of Principal Component Analysis (PCA)](https://builtin.com/data-science/step-step-explanation-principal-component-analysis) by Zakaria Jaadi
| **[Article]** [What Is Principal Component Analysis (PCA) and How It Is Used?](https://www.sartorius.com/en/knowledge/science-snippets/what-is-principal-component-analysis-pca-and-how-it-is-used-507186) by Sartorius Team
| **[Article]** [Principal Components Analysis](https://online.stat.psu.edu/stat508/book/export/html/639) by by Penn State Eberly College of Science
| **[Article]** [Principal Component Analysis – How PCA Algorithms Works, The Concept, Math and Implementation](https://www.machinelearningplus.com/machine-learning/principal-components-analysis-pca-better-explained/) by Selva Prabhakaran
| **[Publication]** [Analysis of a Complex of Statistical Variables into Principal Components](https://psycnet.apa.org/doiLanding?doi=10.1037%2Fh0071325) by Harold Hotelling (Journal of Educational Psychology)
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
|
|
|
|